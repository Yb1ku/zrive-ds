## AN√ÅLISIS Y MEJORA DEL MODELO NO LINEAL 

En este documento se realizar√° un an√°lisis del modelo no lineal desarrollado anteriormente, con el objetivo de entender c√≥mo realiza sus predicciones, detectar posibles problemas de fuga de informaci√≥n y entrenar una versi√≥n reducida y optimizada del modelo. Para ello, se realizar√° un an√°lisis de la importancia de las variables mediante SHAP y, posteriormente, se entrenar√° una nueva versi√≥n del modelo con las variables m√°s importantes, reduciendo as√≠ la complejidad del modelo sin comprometer su rendimiento. Para la b√∫squeda de hiperpar√°metros se llevar√° a cabo un proceso de optimizaci√≥n bayesiana. 


```python
import pandas as pd 
import shap 
import joblib 
import matplotlib.pyplot as plt
import numpy as np 
import json
from types import MethodType
```


```python
import importlib 
import data_processing
importlib.reload(data_processing)
from data_processing import preprocess_data_for_training 

data_path = "../data/feature_frame.csv"
data = pd.read_csv(data_path)
x_train, x_val, x_test, y_train, y_val, y_test = preprocess_data_for_training(data)

model_path = "../models/push_2025_10_27.joblib"
pipe = joblib.load(model_path)
clf = pipe.named_steps['classifier']

n_samples = int(len(x_train) * 0.1)
class_0 = x_train[y_train == 0]
class_1 = x_train[y_train == 1]
n_class_0 = int(n_samples * (len(class_0) / len(x_train)))
n_class_1 = n_samples - n_class_0
x_train_subsample = pd.concat([class_0.sample(n_class_0, random_state=42), class_1.sample(n_class_1, random_state=42)])

explainer = shap.Explainer(clf, x_train_subsample)
shap_values = explainer(x_train_subsample)
shap.summary_plot(shap_values, x_train_subsample)
```

    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    /home/ybiku/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/pickle.py:1718: UserWarning: [12:46:28] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or
    configuration generated by an older version of XGBoost, please export the model by calling
    `Booster.save_model` from that version first, then load it back in current version. See:
    
        https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html
    
    for more details about differences between saving model and serializing.
    
      setstate(state)
    100%|===================| 153889/153981 [13:08<00:00]        


    
![png](model_analysis_files/model_analysis_2_1.png)
    


En t√©rminos absolutos de contribuci√≥n, las variables m√°s influyentes son `global_popularity`, `days_since_purchase_product_type` y `ordered_before`. Esto tiene sentido, pues si un producto es popular es m√°s probable que se compre, y si el usuario compra de forma recurrente el producto, es probable que lo vuelva a comprar. En particular, es importante destacar el efecto que tienen las variables `ordered_before`, `abandoned_before`, `set_as_regular` y `active_snoozed`: valores negativos de estas variables no tienen influencia en el modelo, pero valores positivos empujan fuertemente la predicci√≥n hacia la clase positiva. Es decir, si el cliene ha tenido el producto en el carrito alguna vez, si lo tiene marcado como compra regular o si tiene la notificaci√≥n activada, el modelo interpreta que son se√±ales muy fuertes de que el cliente puede estar interesado en comprar el producto. Hay variables que tienen m√°s importancia en t√©rminos absolutos que esas cuatro variables, pero su efecto en empujar la predicci√≥n hacia la clase positiva merece ser tenido en cuenta. 

#### EXPLICACI√ìN DE ALTO NIVEL DEL FUNCIONAMIENTO DEL MODELO 

El algoritmo se fija principalmente en tres aspectos: **popularidad del producto**, **cu√°ndo fue la √∫ltima vez que el usuario lo compr√≥** y **si el usuario ha comprado el producto anteriormente**. A esa informaci√≥n se le suma el precio del art√≠culo y datos acerca del comportamiento de compra del usuario con ese producto y diferentes art√≠culos del mismo tipo. Por √∫ltimo, el algoritmo tiene en cuenta si el usuario ha interactuado con el producto con anterioridad, mirando si lo ha abandonado en un carrito anterior, si lo tiene marcado como compra regular y si tiene las alertas activadas. Con esa informaci√≥n, el algoritmo determina cu√°l es la probabilidad de que el usuario quiera comprar el producto. 

Para tratar de simplificar el modelo, se entrenar√° un modelo √∫nicamente con las siguientes variables: `global_popularity`, `ordered_before`, `abandoned_before`, `set_as_regular` y `active_snoozed`. Si el modelo reducido obtiene resultados similares, se podr√≠a reducir la complejidad del modelo sin comprometer su rendimiento. 


```python
import train
import data_processing 
importlib.reload(data_processing)
importlib.reload(train)
from train import fit_model
from pathlib import Path
from sklearn.metrics import auc 

BASE_DIR = Path.cwd().resolve().parents[0]
DATA_PATH = BASE_DIR / "data" / "feature_frame.csv"
MODEL_PATH = BASE_DIR / "models"
RANDOM_SEED = 42

# Same hyperparameters as the original model 
model_params = {
        'n_estimators': 300,
        'eta': 0.01,
        'gamma': 5,
        'max_depth': 8
    }

columns_to_use = ['global_popularity', 'ordered_before', 'abandoned_before', 'set_as_regular', 'active_snoozed'] 
columns_to_drop = [col for col in x_train.columns if col not in columns_to_use]

original_model_path = MODEL_PATH / "original_model.joblib"
reduced_model_path = MODEL_PATH / "reduced_model.joblib"

original_model = joblib.load(original_model_path)
reduced_model = joblib.load(reduced_model_path)

_, og_recall_train, og_precision_train, og_recall_val, og_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH, 
    model_params=model_params, name='original_model', save_figure=False)

_, reduced_recall_train, reduced_precision_train, reduced_recall_val, reduced_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH,
    model_params=model_params, columns_to_drop=columns_to_drop,
    name='reduced_model', save_figure=False)

og_auc_train = auc(og_recall_train, og_precision_train)
reduced_auc_train = auc(reduced_recall_train, reduced_precision_train)
og_auc_val = auc(og_recall_val, og_precision_val)
reduced_auc_val = auc(reduced_recall_val, reduced_precision_val)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(og_recall_train, og_precision_train, label='Original - AUC: {:.2f}'.format(og_auc_train), color='blue')
plt.plot(reduced_recall_train, reduced_precision_train, label='Reduced - AUC: {:.2f}'.format(reduced_auc_train), color='red')
plt.title('Train Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(og_recall_val, og_precision_val, label='Original - AUC: {:.2f}'.format(og_auc_val), color='blue')
plt.plot(reduced_recall_val, reduced_precision_val, label='Reduced - AUC: {:.2f}'.format(reduced_auc_val), color='red')
plt.title('Validation Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.tight_layout()
plt.show()
```

    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.
    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - preprocess_data_for_training - Dropping additional columns: ['user_order_seq', 'normalised_price', 'discount_pct', 'count_adults', 'count_children', 'count_babies', 'count_pets', 'people_ex_baby', 'days_since_purchase_variant_id', 'avg_days_to_buy_variant_id', 'std_days_to_buy_variant_id', 'days_since_purchase_product_type', 'avg_days_to_buy_product_type', 'std_days_to_buy_product_type']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.



    
![png](model_analysis_files/model_analysis_6_1.png)
    


Los resultados muestran que, al emplear solo las variables `global_popularity`, `ordered_before`, `abandoned_before`, `set_as_regular` y `active_snoozed`, el modelo muestra un ligero deterioro de su capacidad predictiva. A continuaci√≥n, se entrenar√° una nueva versi√≥n del modelo con las 5 variables m√°s importantes: `global_popularity`, `days_since_purchase_product_type`, `ordered_before`, `normalised_price` y `std_days_to_buy_variant_id`. Con esto, se podr√° ver si las variables `abandoned_before`, `set_as_regular` y `active_snoozed` realmente tienen un peso significativo en las predicciones realizadas por el modelo. 


```python
columns_to_use = ['global_popularity', 'days_since_purchase_product_type', 
                  'ordered_before', 'normalised_price', 'std_days_to_buy_variant_id']
columns_to_drop = [col for col in x_train.columns if col not in columns_to_use]
_, top5_recall_train, top5_precision_train, top5_recall_val, top5_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH,
    model_params=model_params, columns_to_drop=columns_to_drop,
    name='top5_model', save_figure=False)

top5_auc_train = auc(top5_recall_train, top5_precision_train)
top5_auc_val = auc(top5_recall_val, top5_precision_val)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(og_recall_train, og_precision_train, label='Original - AUC: {:.2f}'.format(og_auc_train), color='blue')
plt.plot(top5_recall_train, top5_precision_train, label='Top 5 - AUC: {:.2f}'.format(top5_auc_train), color='green')
plt.title('Train Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(og_recall_val, og_precision_val, label='Original - AUC: {:.2f}'.format(og_auc_val), color='blue')
plt.plot(top5_recall_val, top5_precision_val, label='Top 5 - AUC: {:.2f}'.format(top5_auc_val), color='green')
plt.title('Validation Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.tight_layout()
plt.show()
```

    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - preprocess_data_for_training - Dropping additional columns: ['user_order_seq', 'abandoned_before', 'active_snoozed', 'set_as_regular', 'discount_pct', 'count_adults', 'count_children', 'count_babies', 'count_pets', 'people_ex_baby', 'days_since_purchase_variant_id', 'avg_days_to_buy_variant_id', 'avg_days_to_buy_product_type', 'std_days_to_buy_product_type']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.



    
![png](model_analysis_files/model_analysis_8_1.png)
    


El rendimiento ha disminuido de forma considerable. Ahora se probar√° a entrenar el modelo con las 10 variables m√°s influyentes seg√∫n SHAP. 


```python
top10_columns = shap_values.abs.mean(0).values.argsort()[-10:][::-1]
top10_column_names = x_train.columns[top10_columns].tolist()

columns_to_drop = [col for col in x_train.columns if col not in top10_column_names]
_, top10_recall_train, top10_precision_train, top10_recall_val, top10_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH,
    model_params=model_params, columns_to_drop=columns_to_drop,
    name='top10_model', save_figure=False)

top10_auc_train = auc(top10_recall_train, top10_precision_train)
top10_auc_val = auc(top10_recall_val, top10_precision_val)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(og_recall_train, og_precision_train, label='Original - AUC: {:.2f}'.format(og_auc_train), color='blue')
plt.plot(top10_recall_train, top10_precision_train, label='Top 10 - AUC: {:.2f}'.format(top10_auc_train), color='orange')
plt.title('Train Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(og_recall_val, og_precision_val, label='Original - AUC: {:.2f}'.format(og_auc_val), color='blue')
plt.plot(top10_recall_val, top10_precision_val, label='Top 10 - AUC: {:.2f}'.format(top10_auc_val), color='orange')
plt.title('Validation Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.tight_layout()
plt.show()
```

    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - preprocess_data_for_training - Dropping additional columns: ['abandoned_before', 'active_snoozed', 'set_as_regular', 'count_adults', 'count_children', 'count_babies', 'count_pets', 'people_ex_baby', 'days_since_purchase_variant_id']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.



    
![png](model_analysis_files/model_analysis_10_1.png)
    


Aun empleando las 10 variables m√°s importantes, el modelo reducido sigue obteniendo peores resultados que el modelo original. Ahora, empleando las 13 variables m√°s importantes: 


```python
top13_columns = shap_values.abs.mean(0).values.argsort()[-13:][::-1]
top13_column_names = x_train.columns[top13_columns].tolist()

columns_to_drop = [col for col in x_train.columns if col not in top13_column_names]
_, top13_recall_train, top13_precision_train, top13_recall_val, top13_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH,
    model_params=model_params, columns_to_drop=columns_to_drop,
    name='top13_model', save_figure=False)

top13_auc_train = auc(top13_recall_train, top13_precision_train)
top13_auc_val = auc(top13_recall_val, top13_precision_val)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(og_recall_train, og_precision_train, label='Original - AUC: {:.2f}'.format(og_auc_train), color='blue')
plt.plot(top13_recall_train, top13_precision_train, label='Top 13 - AUC: {:.2f}'.format(top13_auc_train), color='purple')
plt.title('Train Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(og_recall_val, og_precision_val, label='Original - AUC: {:.2f}'.format(og_auc_val), color='blue')
plt.plot(top13_recall_val, top13_precision_val, label='Top 13 - AUC: {:.2f}'.format(top13_auc_val), color='purple')
plt.title('Validation Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.tight_layout()
plt.show()
```

    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - preprocess_data_for_training - Dropping additional columns: ['count_adults', 'count_children', 'count_babies', 'count_pets', 'people_ex_baby', 'days_since_purchase_variant_id']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.



    
![png](model_analysis_files/model_analysis_12_1.png)
    


Los resultados obtenidos con las top-13 variables son pr√°cticamente id√©nticos que los del modelo original (22 variables). Esto demuestra que, aunque las variables m√°s importantes para el modelo sean `global_popularity`, `days_since_purchase_product_type`, `ordered_before`, `normalised_price` y `std_days_to_buy_variant_id`, las variables `abandoned_before`, `set_as_regular` y `active_snoozed` sirven como apoyo para a√±adir un poco de poder predictivo. A este nuevo modelo se le aplicar√° una b√∫squeda de hiperpar√°metros para tratar de sacarle el m√°ximo partido posible. 


```python
from bayes_opt import BayesianOptimization
from sklearn.metrics import average_precision_score
from xgboost import XGBClassifier

model_path = "../models/top13_model.joblib"
model = joblib.load(model_path)

INT_KEYS = ['classifier__n_estimators', 'classifier__max_depth', 'classifier__max_leaves',]

def xgb_evaluation_function(**params):
    for k in INT_KEYS:
        if k in params:
            params[k] = int(round(params[k]))

    model.set_params(**params)

    model.fit(x_train[top13_column_names], y_train,)
    probs = model.predict_proba(x_val[top13_column_names])[:, 1]
    return average_precision_score(y_val, probs)

bounds = {
    'classifier__n_estimators': (40, 300),
    'classifier__learning_rate': (0.01, 0.3),
    'classifier__gamma': (0, 5),
    'classifier__max_depth': (3, 10),
    'classifier__min_child_weight': (1, 10),
    'classifier__max_delta_step': (0, 10),
    'classifier__lambda': (0.01, 10.0),
    'classifier__alpha': (0.01, 10.0), 
    'classifier__scale_pos_weight': (1, 20)
} 

model_bo = BayesianOptimization(
    f=xgb_evaluation_function,
    pbounds=bounds,
    random_state=42,
    verbose=2
)

model_bo.maximize(init_points=5, n_iter=25)
```

    |   iter    |  target   | classi... | classi... | classi... | classi... | classi... | classi... | classi... | classi... | classi... |
    -------------------------------------------------------------------------------------------------------------------------------------
    | [39m1        [39m | [39m0.1929345[39m | [39m137.38043[39m | [39m0.2857071[39m | [39m3.6599697[39m | [39m7.1906093[39m | [39m2.4041677[39m | [39m1.5599452[39m | [39m0.5902552[39m | [39m8.6630996[39m | [39m12.421185[39m |
    | [35m2        [39m | [35m0.1995347[39m | [35m224.09887[39m | [35m0.0159695[39m | [35m4.8495492[39m | [35m8.8270984[39m | [35m2.9110519[39m | [35m1.8182496[39m | [35m1.8422110[39m | [35m3.0493800[39m | [35m10.970372[39m |
    | [39m3        [39m | [39m0.1971078[39m | [39m152.30570[39m | [39m0.0944564[39m | [39m3.0592644[39m | [39m3.9764570[39m | [39m3.6293018[39m | [39m3.6636184[39m | [39m4.5661391[39m | [39m7.8539078[39m | [39m4.7938018[39m |
    | [39m4        [39m | [39m0.1947192[39m | [39m173.70095[39m | [39m0.1818002[39m | [39m0.2322520[39m | [39m7.2528139[39m | [39m2.5347171[39m | [39m0.6505159[39m | [39m9.4893665[39m | [39m9.6566640[39m | [39m16.359549[39m |
    | [35m5        [39m | [35m0.2015842[39m | [35m119.19957[39m | [35m0.0383249[39m | [35m3.4211651[39m | [35m6.0810674[39m | [35m2.0983441[39m | [35m4.9517691[39m | [35m0.3535413[39m | [35m9.0941108[39m | [35m5.9168196[39m |
    | [39m6        [39m | [39m0.1908142[39m | [39m223.98135[39m | [39m0.2200910[39m | [39m3.5095127[39m | [39m7.6396608[39m | [39m2.9029895[39m | [39m3.4594166[39m | [39m1.3688774[39m | [39m1.6519518[39m | [39m8.8477401[39m |
    | [39m7        [39m | [39m0.1978037[39m | [39m98.509505[39m | [39m0.2372564[39m | [39m0.2829666[39m | [39m5.9434336[39m | [39m8.3434316[39m | [39m9.9603129[39m | [39m8.1070355[39m | [39m4.0081563[39m | [39m4.8655204[39m |
    | [39m8        [39m | [39m0.1929836[39m | [39m225.33965[39m | [39m0.1785384[39m | [39m2.5938673[39m | [39m7.5151134[39m | [39m2.2967828[39m | [39m4.1316388[39m | [39m6.2786456[39m | [39m5.4652869[39m | [39m16.536794[39m |
    | [39m9        [39m | [39m0.1987703[39m | [39m162.81650[39m | [39m0.0608018[39m | [39m0.1670429[39m | [39m8.7550971[39m | [39m8.6746837[39m | [39m5.4338392[39m | [39m9.9782367[39m | [39m4.5023875[39m | [39m11.295923[39m |
    | [39m10       [39m | [39m0.1933161[39m | [39m153.33689[39m | [39m0.1598873[39m | [39m1.0266052[39m | [39m8.4577261[39m | [39m7.4656326[39m | [39m0.8877080[39m | [39m6.7887699[39m | [39m6.1049767[39m | [39m16.344677[39m |
    | [39m11       [39m | [39m0.2002987[39m | [39m211.30379[39m | [39m0.0810134[39m | [39m4.4255492[39m | [39m9.4986932[39m | [39m9.2391038[39m | [39m1.2936352[39m | [39m9.7678323[39m | [39m3.1171694[39m | [39m9.3262435[39m |
    | [39m12       [39m | [39m0.1946291[39m | [39m294.69858[39m | [39m0.0545674[39m | [39m4.9484076[39m | [39m9.1335409[39m | [39m9.9604215[39m | [39m3.6584130[39m | [39m8.1190709[39m | [39m1.5284915[39m | [39m19.455649[39m |
    | [39m13       [39m | [39m0.1966646[39m | [39m192.73258[39m | [39m0.1265210[39m | [39m3.3804963[39m | [39m4.8781955[39m | [39m3.2979447[39m | [39m3.9749549[39m | [39m3.4931646[39m | [39m9.2735255[39m | [39m12.984056[39m |
    | [39m14       [39m | [39m0.1963338[39m | [39m169.75761[39m | [39m0.1787481[39m | [39m4.8900102[39m | [39m8.5394754[39m | [39m5.9963068[39m | [39m3.4775140[39m | [39m7.2820514[39m | [39m5.7246505[39m | [39m7.1967916[39m |
    | [39m15       [39m | [39m0.1640604[39m | [39m299.18640[39m | [39m0.2373864[39m | [39m0.5663633[39m | [39m8.5806962[39m | [39m2.9612823[39m | [39m6.9217129[39m | [39m1.0002397[39m | [39m2.9114094[39m | [39m11.274117[39m |
    | [39m16       [39m | [39m0.1915749[39m | [39m193.09674[39m | [39m0.2821463[39m | [39m4.1359161[39m | [39m7.3522361[39m | [39m9.2515476[39m | [39m5.0933098[39m | [39m6.3749032[39m | [39m8.5477845[39m | [39m15.671814[39m |
    | [39m17       [39m | [39m0.1973264[39m | [39m205.26905[39m | [39m0.1274596[39m | [39m4.8920199[39m | [39m4.7409023[39m | [39m8.8476097[39m | [39m5.7475829[39m | [39m3.4657341[39m | [39m4.9895228[39m | [39m15.163906[39m |
    | [39m18       [39m | [39m0.1940597[39m | [39m245.67814[39m | [39m0.2214011[39m | [39m0.5533457[39m | [39m6.3721780[39m | [39m1.2308336[39m | [39m8.2535692[39m | [39m9.4511018[39m | [39m8.8162486[39m | [39m5.1544700[39m |
    | [39m19       [39m | [39m0.1933475[39m | [39m52.555031[39m | [39m0.1869846[39m | [39m1.3675742[39m | [39m3.2556432[39m | [39m1.7634743[39m | [39m2.9672940[39m | [39m1.2850559[39m | [39m1.0141458[39m | [39m6.3100051[39m |
    | [39m20       [39m | [39m0.1953224[39m | [39m297.58796[39m | [39m0.0815277[39m | [39m1.3446055[39m | [39m3.5125329[39m | [39m3.9378738[39m | [39m8.1407379[39m | [39m0.6806597[39m | [39m1.9360679[39m | [39m15.285091[39m |
    | [39m21       [39m | [39m0.1965104[39m | [39m204.44480[39m | [39m0.1188133[39m | [39m0.3670894[39m | [39m3.9834471[39m | [39m3.1360190[39m | [39m2.0494354[39m | [39m6.4806726[39m | [39m7.3959268[39m | [39m13.298207[39m |
    | [39m22       [39m | [39m0.1906168[39m | [39m40.333976[39m | [39m0.2966193[39m | [39m2.5953886[39m | [39m3.3903569[39m | [39m9.9915237[39m | [39m9.5796226[39m | [39m8.7294550[39m | [39m5.3577135[39m | [39m3.7995034[39m |
    | [39m23       [39m | [39m0.1887222[39m | [39m290.58876[39m | [39m0.1987383[39m | [39m0.5846387[39m | [39m6.6230007[39m | [39m3.2628571[39m | [39m8.1223233[39m | [39m6.7533618[39m | [39m5.0343476[39m | [39m7.7811189[39m |
    | [39m24       [39m | [39m0.1975907[39m | [39m179.27188[39m | [39m0.1407193[39m | [39m1.8431279[39m | [39m7.1961365[39m | [39m7.7322089[39m | [39m1.4497612[39m | [39m8.2413923[39m | [39m8.3213104[39m | [39m11.880108[39m |
    | [39m25       [39m | [39m0.0654811[39m | [39m99.470521[39m | [39m0.0140755[39m | [39m0.2702653[39m | [39m5.1689987[39m | [39m1.5786692[39m | [39m0.1950079[39m | [39m7.2836218[39m | [39m5.6818835[39m | [39m10.725102[39m |
    | [39m26       [39m | [39m0.1996051[39m | [39m214.89250[39m | [39m0.01     [39m | [39m5.0      [39m | [39m10.0     [39m | [39m5.9890270[39m | [39m0.0      [39m | [39m0.4657164[39m | [39m10.0     [39m | [39m13.632967[39m |
    | [35m27       [39m | [35m0.2021298[39m | [35m204.81411[39m | [35m0.01     [39m | [35m5.0      [39m | [35m10.0     [39m | [35m7.7784773[39m | [35m0.0      [39m | [35m0.6637593[39m | [35m8.3413277[39m | [35m3.4144518[39m |
    | [39m28       [39m | [39m0.2012771[39m | [39m209.16217[39m | [39m0.01     [39m | [39m5.0      [39m | [39m10.0     [39m | [39m2.8573787[39m | [39m9.8992529[39m | [39m5.6064836[39m | [39m10.0     [39m | [39m5.0428536[39m |
    | [39m29       [39m | [39m0.2008824[39m | [39m201.38211[39m | [39m0.01     [39m | [39m5.0      [39m | [39m10.0     [39m | [39m1.5908138[39m | [39m4.9645839[39m | [39m6.6073249[39m | [39m0.01     [39m | [39m2.7623058[39m |
    | [39m30       [39m | [39m0.1822118[39m | [39m202.84834[39m | [39m0.01     [39m | [39m5.0      [39m | [39m3.0      [39m | [39m10.0     [39m | [39m5.7760288[39m | [39m10.0     [39m | [39m8.0085818[39m | [39m1.0      [39m |
    =====================================================================================================================================


Las variables empleadas se pueden dividir en dos grupos: 
1. **Variables sobre el producto**: `global_popularity`, `normalised_price`, `discount_pct`. 
2. **Variables sobre el usuario**: `days_since_purchase_product_type`, `ordered_before`, `std_days_to_buy_variant_id`, `user_order_seq`, `std_days_to_buy_product_type`, `avg_days_to_buy_product_type`, `avg_days_to_buy_variant_id`, `abandoned_before`, `set_as_regular`, `active_snoozed`. 


```python
best_params = model_bo.max['params']
for k in INT_KEYS:
    if k in best_params:
        best_params[k] = int(round(best_params[k]))

cleaned_params = {k.replace('classifier__', ''): v for k, v in best_params.items()}

_, bo_recall_train, bo_precision_train, bo_recall_val, bo_precision_val = train.fit_model(
    data_path=DATA_PATH, model_path=MODEL_PATH,
    model_params=cleaned_params, columns_to_drop=[col for col in x_train.columns if col not in top13_column_names],
    name='top13_bo_model', save_figure=False)

bo_auc_train = auc(bo_recall_train, bo_precision_train)
bo_auc_val = auc(bo_recall_val, bo_precision_val)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(og_recall_train, og_precision_train, label='Original - AUC: {:.2f}'.format(og_auc_train), color='blue')
plt.plot(bo_recall_train, bo_precision_train, label='Top 13 BO - AUC: {:.2f}'.format(bo_auc_train), color='brown')
plt.title('Train Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(og_recall_val, og_precision_val, label='Original - AUC: {:.2f}'.format(og_auc_val), color='blue')
plt.plot(bo_recall_val, bo_precision_val, label='Top 13 BO - AUC: {:.2f}'.format(bo_auc_val), color='brown')
plt.title('Validation Set Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.tight_layout()
plt.show()
```

    [INFO] - fit_model - Loading data...
    [INFO] - fit_model - Preprocessing data...
    [INFO] - preprocess_data_for_training - Filtering orders with at least 5 items...
    [INFO] - preprocess_data_for_training - Splitting data into train, validation, and test sets...
    [INFO] - preprocess_data_for_training - Applying common transformations...
    [INFO] - preprocess_data_for_training - Dropping categorical columns: ['product_type', 'vendor']...
    [INFO] - preprocess_data_for_training - Dropping additional columns: ['count_adults', 'count_children', 'count_babies', 'count_pets', 'people_ex_baby', 'days_since_purchase_variant_id']...
    [INFO] - create_pipeline - Creating the model pipeline...
    [INFO] - fit_model - Training the model...
    [INFO] - fit_model - Saving the trained model in /home/ybiku/projects/zrive/src/zrive/models...
    [INFO] - fit_model - Model training and saving completed.



    
![png](model_analysis_files/model_analysis_16_1.png)
    



```python
print("Best hyperparameters found by Bayesian Optimization:")
for param, value in cleaned_params.items():
    print(f"{param}: {value}")
```

    Best hyperparameters found by Bayesian Optimization:
    n_estimators: 205
    learning_rate: 0.01
    gamma: 5.0
    max_depth: 10
    min_child_weight: 7.778477304083368
    max_delta_step: 0.0
    lambda: 0.6637593018479666
    alpha: 8.341327758939094
    scale_pos_weight: 3.414451814357018


Los resultados son pr√°cticamente iguales, por lo que se ha conseguido reducir la complejidad del modelo sin comprometer su rendimiento. 

Por tanto, el modelo final consiste en un XGBoost con los siguientes hiperpar√°metros: 
- `n_estimators`: 205
- `learning_rate`: 0.01
- `gamma`: 5
- `max_depth`: 10
- `min_child_weight`: 7.778477304083368
- `max_delta_step`: 0
- `lambda`: 0.6637593018479666
- `classifier_alpha`: 8.341327758939094
- `scale_pos_weight`: 3.414451814357018


```python
n_samples = int(len(x_train) * 0.1)
class_0 = x_train[y_train == 0]
class_1 = x_train[y_train == 1]
n_class_0 = int(n_samples * (len(class_0) / len(x_train)))
n_class_1 = n_samples - n_class_0
x_train_subsample = pd.concat([class_0.sample(n_class_0, random_state=42), class_1.sample(n_class_1, random_state=42)])
model_path = "../models/top13_bo_model.joblib"
model = joblib.load(model_path)
explainer = shap.Explainer(model.named_steps['classifier'], x_train_subsample[top13_column_names])
shap_values = explainer(x_train_subsample[top13_column_names])
shap.plots.beeswarm(shap_values)
```

     99%|===================| 152906/153981 [01:38<00:00]        


    
![png](model_analysis_files/model_analysis_20_1.png)
    


De este gr√°fico se puede extraer informaci√≥n de c√≥mo emplea el modelo las variables: 

- `avg_days_to_buy_variant_id` se muestra como la variable m√°s importante, pero no parece tener una relaci√≥n clara con la variable objetivo. Valores m√°s peque√±os (esto es, si el producto es comprado muy frecuentemente) empuja un poco la predicci√≥n hacia la clase positiva. Sin embargo, la falta de relaci√≥n clara entre valor de la variable y output puede sugerir que su influencia no es lineal, sino que viene dada en formas no lineales o en combinaci√≥n con otras variables. 
- En cuanto a `ordered_before`, se ve que valores positivos de esta variable empujan la predicci√≥n hacia la clase positiva, lo que muestra que si el usuario ha comprado antes el producto, el modelo interpreta que es probable que lo vuelva a comprar. 
- Para `days_since_purchase_product_type`, cuanto m√°s reciente sea la √∫ltima compra realizada, menor es la probabilidad de que el usuario vuelva a comprar el producto (parece l√≥gico, pues si lo ha comprado recientemente no va a necesitar comprarlo ahora). 
- El resto de variables no parecen mostrar una relaci√≥n evidente, por lo que su influencia vendr√° dada por relaciones no lineales o combinaciones entre ellas. 

En definitiva, el modelo toma el promedio de d√≠as que el usuario tarda en comprar el producto como variable m√°s importante, seguido de si el usuario ha comprado el producto anteriormente. El resto de variables complementan la informaci√≥n. 

Algo a destacar es que este nuevo modelo reducido no emplea las variables de la misma forma que el modelo original. En el original, las variables `abandoned_before`, `set_as_regular` y `active_snoozed` presentaban una influencia clara: valores positivos empujaban la predicci√≥n hacia la clase positiva. Sin embargo, en este nuevo modelo no ocurre lo mismo. En general, la importancia de las variables es menos clara e interpretable que en el modelo original, lo que desde una perspectiva de negocio podr√≠a llevar a que se deposite menos confianza en √©l, pese a que sea computacionalmente menos costoso y no reduzca el rendimiento de la herramienta. De cara a una presentaci√≥n de resultados, habr√≠a que plantear esta cuesti√≥n para decidir qu√© modelo escoger, el original (m√°s interpretable) o el reducido (m√°s optimizado). 
